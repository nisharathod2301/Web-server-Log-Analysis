{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Server Log Analysis\n",
    "\n",
    "## Outline\n",
    "### History \n",
    "I started this project at 2015. That time, with Spark 1.4,  the RDD was the only structure to use. The main focus of the project was exploring the basic functionality of pypark and possibilities of exploratory analysis for distributed data.\n",
    "The data used in 2015 [http://ita.ee.lbl.gov/html/contrib/Calgary-HTTP.html](http://ita.ee.lbl.gov/html/contrib/Calgary-HTTP.html) do not exist anymore. \n",
    "\n",
    "### Focus in 2022\n",
    "I found many logs at [https://www.sec.gov/dera/data/edgar-log-file-data-set.html](https://www.sec.gov/dera/data/edgar-log-file-data-set.html)\n",
    "Here I take the 2003 data, because the file is relatively small, about 3-4M. In 2017 the file size is about 300-400M.\n",
    "\n",
    "Now the project has a new focus. This Jupyter Notebok and production scripts for batch processing serve as investigation steps. The main goal is stream processing of logs wiht Spark Streams and scheduling with Airflow. \n",
    "\n",
    "### Why RDD und Spark Streams?\n",
    "For historical reasons I decided to keep the project focused on RDDs and Spark Streams. They are suitable for unstructured data. The processing described here will bring the structure to those.\n",
    "\n",
    "For initially sructured data the better tools are DataFrames and StructuredStreams. For those I will find a new playground.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collecting data\n",
    "\n",
    "### Read the file from URL and write it on hard drive\n",
    "\n",
    "In 2015 I assumed that the log file is already downloaded and in the current directory as TXT file.\n",
    "This I load ZIP from url and unzip locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io, os, requests, zipfile\n",
    "from pyspark.sql import Row\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-18T19:25:54.472167Z",
     "start_time": "2022-01-18T19:25:54.467235Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qtr 1\n",
      "./data/log20030303.csv\n"
     ]
    }
   ],
   "source": [
    "date = \"20030303\"\n",
    "year = date[:4]\n",
    "Qtr = (int(date[4:6])-1)//3+1; print('Qtr', Qtr)\n",
    "#\n",
    "data_directory = './data/'\n",
    "csv_file = os.path.join(data_directory, f'log{date}.csv')\n",
    "print(csv_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-18T19:25:54.472167Z",
     "start_time": "2022-01-18T19:25:54.467235Z"
    }
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(csv_file):\n",
    "    zip_file_url = f'http://www.sec.gov/dera/data/Public-EDGAR-log-file-data/{year}/Qtr{Qtr:d}/log{date}.zip'\n",
    "    r = requests.get(zip_file_url)\n",
    "    z = zipfile.ZipFile(io.BytesIO(r.content))\n",
    "    z.extractall(data_directory)\n",
    "else:\n",
    "    pass  # The file is already there"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the file into RDD\n",
    "\n",
    "In Jupyter notebook the SparkSession `spark` and the SparkContext `sc` are created for us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://luck.fritz.box:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fba6cf7fb20>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://luck.fritz.box:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=PySparkShell>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create \"logFileRDD\" of the file by reading it as a collection of lines. For a start we have a look at the first several lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-18T12:58:51.291892Z",
     "start_time": "2022-01-18T12:58:51.090083Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ip,date,time,zone,cik,accession,extention,code,size,idx,norefer,noagent,find,crawler,browser',\n",
       " '208.252.214.jbf,2003-03-03,00:00:00,500.0,919642.0,0000891836-02-000291,-index.htm,304.0,,1.0,0.0,0.0,1.0,0.0,mie',\n",
       " '66.48.138.ach,2003-03-03,00:00:02,500.0,1084230.0,0001104659-03-003192,.txt,200.0,330570.0,0.0,1.0,0.0,0.0,0.0,',\n",
       " '209.172.247.haf,2003-03-03,00:00:04,500.0,78003.0,0000914121-02-001461,pf121702-8k.txt,200.0,2527.0,0.0,0.0,0.0,9.0,0.0,win',\n",
       " '32.100.133.aej,2003-03-03,00:00:05,500.0,772197.0,0000950144-02-004587,g75920def14a.txt,200.0,74288.0,0.0,0.0,0.0,9.0,0.0,win']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assert os.path.exists(csv_file)\n",
    "logFileRDD = sc.textFile(csv_file, 4).cache()\n",
    "#logFileRDD = spark.read.option(\"header\", \"true\").format(\"csv\")\n",
    "logFileRDD.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The file has a header now, which should be stripped. The format of the file is diffrent with 15 fields (variables). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-18T12:58:53.298371Z",
     "start_time": "2022-01-18T12:58:53.231720Z"
    }
   },
   "outputs": [],
   "source": [
    "header = logFileRDD.first() #extract header\n",
    "logFileRDD = logFileRDD.filter(lambda row: row != header)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing\n",
    "\n",
    "### Read docuemntation for expected format\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-18T13:15:42.571713Z",
     "start_time": "2022-01-18T13:15:42.564039Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"800\"\n",
       "            height=\"1050\"\n",
       "            src=\"images/EDGAR_variables_FINAL.pdf\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f50ad5f5c70>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import IFrame\n",
    "IFrame(\"images/EDGAR_variables_FINAL.pdf\", width=800, height=1050)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We briefly summarize the content of the fields\n",
    "1. ip\n",
    "2. date - Apache log file date (yyyy-mm-dd)\n",
    "3. time - Apache log file time (hh:mm:ss)\n",
    "4. zone - Apache log file zone\n",
    "5. cik - SEC Central Index Key (CIK) associated with the document requested\n",
    "6. accession - accession SEC document accession number associated with the document requested\n",
    "7. extention - filename\n",
    "8. code - Apache log file status code for the request\n",
    "9. size - document file size\n",
    "10. idx - takes on a value of 1 if the requester landed on the index page of a set of documents\n",
    "11. norefer\n",
    "12. noagent\n",
    "13. find\n",
    "14. crawler\n",
    "15. browser\n",
    "\n",
    "Let us parse each line of our RDD accordingly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct regular expression for parsing\n",
    "\n",
    "the common regexp patterns can be found here [https://regexpattern.com/date-time/](https://regexpattern.com/date-time/) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^(\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\w{3}),(\\d{4}\\-\\d{2}\\-\\d{2}),(\\d{2}:\\d{2}:\\d{2}),(\\d+\\.?\\d*),(\\d+\\.?\\d*),([\\w\\-]+),([\\w\\-\\.]+),(\\d+\\.?\\d*),(\\d+\\.?\\d*),([01]\\.?0?),([01]\\.?0?),([01]\\.?0?),(1?[0-9]\\.?0?),([01]\\.?0?),([a-z]{3})$\n"
     ]
    }
   ],
   "source": [
    "# A regular expression pattern to extract fields from the log line\n",
    "re_ip = '\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\w{3}'\n",
    "re_date = '\\d{4}\\-\\d{2}\\-\\d{2}'\n",
    "re_time = '\\d{2}:\\d{2}:\\d{2}'\n",
    "re_zone = '\\d+\\.?\\d*'\n",
    "re_cik = '\\d+\\.?\\d*'\n",
    "re_accession = '[\\w\\-]+'\n",
    "re_filename = '[\\w\\-\\.]+'\n",
    "re_code = '\\d+\\.?\\d*'\n",
    "re_size = '\\d+\\.?\\d*'\n",
    "re_idx = '[01]\\.?0?' \n",
    "re_norefer = '[01]\\.?0?'\n",
    "re_noagent = '[01]\\.?0?' \n",
    "re_find = '1?[0-9]\\.?0?'\n",
    "re_crawler = '[01]\\.?0?' \n",
    "re_browser = '[a-z]{3}'\n",
    "#\n",
    "LOG_PATTERN_EDGAR = f'^({re_ip:s}),({re_date:s}),({re_time:s}),({re_zone:s}),({re_cik:s}),\\\n",
    "({re_accession:s}),({re_filename:s}),({re_code:s}),({re_size:s}),({re_idx:s}),\\\n",
    "({re_norefer:s}),({re_noagent:s}),({re_find:s}),({re_crawler:s}),({re_browser:s})$'\n",
    "print(LOG_PATTERN_EDGAR)\n",
    "pattern=re.compile(LOG_PATTERN_EDGAR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# answer from regex101.com\n",
    "# ^(\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\w{3}),(\\d{4}\\-\\d{2}\\-\\d{2}),(\\d{2}:\\d{2}:\\d{2}),(\\d+\\.?\\d*),(\\d+\\.?\\d*),([\\w\\-]+),([\\w\\-\\.]+),(\\d+\\.?\\d*),(\\d+\\.?\\d*),([01]\\.?0?),([01]\\.?0?),([01]\\.?0?),(1?[0-9]\\.?0?),([01]\\.?0?),([a-z]{3})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.time(14, 2, 13)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check correct pattern for datetime - we neglect the time zone shift for the moment\n",
    "#import datetime\n",
    "#stri = \"24/Oct/1994:13:41:41 -0600\"\n",
    "#dt = datetime.datetime.strptime(stri[:20], \"%d/%b/%Y:%H:%M:%S\")\n",
    "#print(dt)\n",
    "from datetime import datetime\n",
    "datetime.strptime('2003-10-03', \"%Y-%m-%d\").date()\n",
    "datetime.strptime('14:02:13', \"%H:%M:%S\").time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will parse each text line into `pyspark.sql.Row`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseApacheLogLine(logline):\n",
    "    \"\"\" Parse a line in the Apache Common Log format\n",
    "    Inputs:\n",
    "        logline (str): a line of text in the Apache Common Log format\n",
    "    Outputs:\n",
    "        tuple: either a dictionary containing the parts of the Apache Access Log and 1,\n",
    "               or the original invalid log line and 0\n",
    "    \"\"\"\n",
    "    match = re.search(pattern, logline)\n",
    "    if match is None:   # failed match\n",
    "        #print('failed  ', logline)\n",
    "        return (logline, 0)\n",
    "    \n",
    "    #size_field = match.group(9)\n",
    "    #if size_field == '-':\n",
    "    #    size = float(0)\n",
    "    #else:\n",
    "    #    size = float(match.group(9))\n",
    "    \n",
    "    parsed_row = Row(\n",
    "        ip          = match.group(1),\n",
    "        date = datetime.strptime(match.group(2), \"%Y-%m-%d\").date(),\n",
    "        time       = datetime.strptime(match.group(3), \"%H:%M:%S\").time(),\n",
    "        zone        = match.group(4),\n",
    "        cik      = match.group(5),\n",
    "        accesion      = match.group(6),\n",
    "        filename = match.group(7),\n",
    "        response_code = int(float(match.group(8))),\n",
    "        content_size  = match.group(9), # size,\n",
    "        idx = bool(match.group(10)),\n",
    "        browser = match.group(15)\n",
    "    )\n",
    "    #print('parsed   ', logline)\n",
    "    \n",
    "    return (parsed_row, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will keep track of correctly parsed and failed logs are returned those as two RDDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def access_fail_logs(parsed_logs):\n",
    "    \"\"\" Read and parse log file, print a 20-sample of failed log-lines\n",
    "    Inputs:\n",
    "        parsed_logs (RDD): an RDD obtained via parseApacheLogLine(...)\n",
    "    Outputs:\n",
    "        tuple of RDDs: access_logs, failed_logs\n",
    "    \"\"\"\n",
    "    access_logs = (parsed_logs\n",
    "                   .filter(lambda s: s[1] == 1)\n",
    "                   .map(lambda s: s[0])\n",
    "                   .cache())\n",
    "\n",
    "    failed_logs = (parsed_logs\n",
    "                   .filter(lambda s: s[1] == 0)\n",
    "                   .map(lambda s: s[0]))\n",
    "    failed_logs_count = failed_logs.count()\n",
    "    if failed_logs_count > 0:\n",
    "        print(f'Number of invalid logline: {failed_logs.count():d}')\n",
    "        for line in failed_logs.take(20):\n",
    "            print(f'Invalid logline: {line}')\n",
    "\n",
    "    print(f'Read {parsed_logs.count():d} lines, successfully parsed { access_logs.count():d} lines, \\\n",
    "           failed to parse {failed_logs.count():d} lines')\n",
    "    return access_logs, failed_logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that our functions are ready let us start with parsing lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['208.252.214.jbf,2003-03-03,00:00:00,500.0,919642.0,0000891836-02-000291,-index.htm,304.0,,1.0,0.0,0.0,1.0,0.0,mie',\n",
       " '66.48.138.ach,2003-03-03,00:00:02,500.0,1084230.0,0001104659-03-003192,.txt,200.0,330570.0,0.0,1.0,0.0,0.0,0.0,',\n",
       " '209.172.247.haf,2003-03-03,00:00:04,500.0,78003.0,0000914121-02-001461,pf121702-8k.txt,200.0,2527.0,0.0,0.0,0.0,9.0,0.0,win']"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#logFileRDD.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Row(ip='209.172.247.haf', date=datetime.date(2003, 3, 3), time=datetime.time(0, 0, 4), zone='500.0', cik='78003.0', accesion='0000914121-02-001461', filename='pf121702-8k.txt', response_code=200, content_size=2527.0, idx=True, browser='win'),\n",
       " 1)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#parseApacheLogLine('209.172.247.haf,2003-03-03,00:00:04,500.0,78003.0,0000914121-02-001461,pf121702-8k.txt,200.0,2527.0,0.0,0.0,0.0,9.0,0.0,win')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsedLogsRDD = logFileRDD.map(parseApacheLogLine).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('208.252.214.jbf,2003-03-03,00:00:00,500.0,919642.0,0000891836-02-000291,-index.htm,304.0,,1.0,0.0,0.0,1.0,0.0,mie',\n",
       "  0)]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#parsedLogsRDD.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of invalid logline: 48160\n",
      "Invalid logline: 208.252.214.jbf,2003-03-03,00:00:00,500.0,919642.0,0000891836-02-000291,-index.htm,304.0,,1.0,0.0,0.0,1.0,0.0,mie\n",
      "Invalid logline: 66.48.138.ach,2003-03-03,00:00:02,500.0,1084230.0,0001104659-03-003192,.txt,200.0,330570.0,0.0,1.0,0.0,0.0,0.0,\n",
      "Invalid logline: 208.252.214.jbf,2003-03-03,00:00:07,500.0,919642.0,0000891836-02-000291,sc0167-02d.txt,304.0,,0.0,0.0,0.0,9.0,0.0,mie\n",
      "Invalid logline: 66.48.138.ach,2003-03-03,00:00:21,500.0,1004980.0,0001004980-03-000014,.txt,200.0,354518.0,0.0,1.0,0.0,0.0,0.0,\n",
      "Invalid logline: 66.48.138.ach,2003-03-03,00:01:05,500.0,78239.0,0000950136-03-000445,.txt,200.0,476664.0,0.0,1.0,0.0,0.0,0.0,\n",
      "Invalid logline: 149.77.10.fed,2003-03-03,00:01:10,500.0,789019.0,0001032210-02-001614,-index.htm,304.0,,1.0,0.0,0.0,1.0,0.0,win\n",
      "Invalid logline: 66.48.138.ach,2003-03-03,00:01:14,500.0,810136.0,0000810136-03-000008,.txt,200.0,85869.0,0.0,1.0,0.0,0.0,0.0,\n",
      "Invalid logline: 149.77.10.fed,2003-03-03,00:01:16,500.0,789019.0,0001032210-02-001614,d10q.htm,304.0,,0.0,0.0,0.0,9.0,0.0,win\n",
      "Invalid logline: 66.48.138.ach,2003-03-03,00:01:17,500.0,350426.0,0000899243-03-000389,.txt,200.0,21238.0,0.0,1.0,0.0,0.0,0.0,\n",
      "Invalid logline: 66.48.138.ach,2003-03-03,00:01:20,500.0,350426.0,0000899243-03-000399,.txt,200.0,24378.0,0.0,1.0,0.0,0.0,0.0,\n",
      "Invalid logline: 66.48.138.ach,2003-03-03,00:01:23,500.0,878748.0,0000950135-03-001362,.txt,200.0,4758.0,0.0,1.0,0.0,0.0,0.0,\n",
      "Invalid logline: 66.48.138.ach,2003-03-03,00:01:30,500.0,763901.0,0000950144-03-002378,.txt,200.0,119186.0,0.0,1.0,0.0,0.0,0.0,\n",
      "Invalid logline: 61.115.76.jbf,2003-03-03,00:01:36,500.0,1172302.0,0000950168-03-000562,.txt,200.0,3250666.0,0.0,1.0,0.0,0.0,0.0,\n",
      "Invalid logline: 66.48.138.ach,2003-03-03,00:01:36,500.0,79879.0,0000927016-03-000861,.txt,200.0,16760.0,0.0,1.0,0.0,0.0,0.0,\n",
      "Invalid logline: 66.48.138.ach,2003-03-03,00:01:38,500.0,884905.0,0000950159-03-000146,.txt,200.0,5576.0,0.0,1.0,0.0,0.0,0.0,\n",
      "Invalid logline: 66.48.138.ach,2003-03-03,00:01:44,500.0,899881.0,0000950131-03-000815,.txt,200.0,105473.0,0.0,1.0,0.0,0.0,0.0,\n",
      "Invalid logline: 66.48.138.ach,2003-03-03,00:01:50,500.0,906551.0,0001012870-03-000853,.txt,200.0,15642.0,0.0,1.0,0.0,0.0,0.0,\n",
      "Invalid logline: 66.48.138.ach,2003-03-03,00:01:52,500.0,1037949.0,0001047469-03-006343,.txt,200.0,13224.0,0.0,1.0,0.0,0.0,0.0,\n",
      "Invalid logline: 66.48.138.ach,2003-03-03,00:01:55,500.0,1029506.0,0001029506-03-000015,.txt,200.0,6887.0,0.0,1.0,0.0,0.0,0.0,\n",
      "Invalid logline: 66.48.138.ach,2003-03-03,00:01:56,500.0,858558.0,0000858558-03-000024,.txt,200.0,9051.0,0.0,1.0,0.0,0.0,0.0,\n",
      "Read 163455 lines, successfully parsed 115295 lines,            failed to parse 48160 lines\n"
     ]
    }
   ],
   "source": [
    "accessLogsRDD, failedLogsRDD = access_fail_logs(parsedLogsRDD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many lines that failed to parse. Some of then are because of missing content_size and other because of the missing browser. \n",
    "\n",
    "Before dealing with this issue, let us check if parsed results are meaningful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore parsed data\n",
    "\n",
    "How can we be sure that our parsing delivered meaningful results? Let us have a look at the unique values of the \"method\", \"response code\", and \"protocol\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response codes are [200, 206, 302, 404]\n",
      "Browsers are ['iem', 'lin', 'mac', 'mie', 'opr', 'win']\n"
     ]
    }
   ],
   "source": [
    "def distinct_responsecodes_browsers(accessLogsRDD):\n",
    "    \"\"\"\n",
    "    Prints distinct values for  response codes and browsers\n",
    "    Inputs:\n",
    "        accessLogsRDD \n",
    "    \"\"\"\n",
    "    ResponseCodesRDD = accessLogsRDD.map(lambda log: log.response_code)\n",
    "    uniqueResponseCodesRDD = ResponseCodesRDD.distinct()\n",
    "    print(\"Response codes are\", sorted(uniqueResponseCodesRDD.collect()))\n",
    "    BrowserRDD = accessLogsRDD.map(lambda log: log.browser)\n",
    "    uniqueBrowserRDD = BrowserRDD.distinct()\n",
    "    print(\"Browsers are\", sorted(uniqueBrowserRDD.collect()))\n",
    "    #return MethodsRDD, ResponseCodesRDD, ProtocolsRDD\n",
    "    return None\n",
    "\n",
    "distinct_responsecodes_browsers(accessLogsRDD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adjust parsing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^(\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\w{3}),(\\d{4}\\-\\d{2}\\-\\d{2}),(\\d{2}:\\d{2}:\\d{2}),(\\d+\\.?\\d*),(\\d+\\.?\\d*),([\\w\\-]+),([\\w\\-\\.]+),(\\d+\\.?\\d*),(\\d*\\.?\\d*),([01]\\.?0?),([01]\\.?0?),([01]\\.?0?),(1?[0-9]\\.?0?),([01]\\.?0?),([a-z]{0,3})$\n"
     ]
    }
   ],
   "source": [
    "# A regular expression pattern to extract fields from the log line\n",
    "re_ip = '\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\w{3}'\n",
    "re_date = '\\d{4}\\-\\d{2}\\-\\d{2}'\n",
    "re_time = '\\d{2}:\\d{2}:\\d{2}'\n",
    "re_zone = '\\d+\\.?\\d*'\n",
    "re_cik = '\\d+\\.?\\d*'\n",
    "re_accession = '[\\w\\-]+'\n",
    "re_filename = '[\\w\\-\\.]+'\n",
    "re_code = '\\d+\\.?\\d*'\n",
    "re_size = '\\d*\\.?\\d*'\n",
    "re_idx = '[01]\\.?0?' \n",
    "re_norefer = '[01]\\.?0?'\n",
    "re_noagent = '[01]\\.?0?' \n",
    "re_find = '1?[0-9]\\.?0?'\n",
    "re_crawler = '[01]\\.?0?' \n",
    "re_browser = '[a-z]{0,3}'\n",
    "#\n",
    "LOG_PATTERN_EDGAR = f'^({re_ip:s}),({re_date:s}),({re_time:s}),({re_zone:s}),({re_cik:s}),\\\n",
    "({re_accession:s}),({re_filename:s}),({re_code:s}),({re_size:s}),({re_idx:s}),\\\n",
    "({re_norefer:s}),({re_noagent:s}),({re_find:s}),({re_crawler:s}),({re_browser:s})$'\n",
    "print(LOG_PATTERN_EDGAR)\n",
    "pattern=re.compile(LOG_PATTERN_EDGAR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseApacheLogLine(logline):\n",
    "    \"\"\" Parse a line in the Apache Common Log format\n",
    "    Inputs:S\n",
    "        logline (str): a line of text in the Apache Common Log format\n",
    "    Outputs:\n",
    "        tuple: either a dictionary containing the parts of the Apache Access Log and 1,\n",
    "               or the original invalid log line and 0\n",
    "    \"\"\"\n",
    "    match = re.search(pattern, logline)\n",
    "    if match is None:   # failed match\n",
    "        print('failed  ', logline)\n",
    "        return (logline, 0)\n",
    "    \n",
    "    size_field = match.group(9)\n",
    "    if size_field:\n",
    "        size = float(match.group(9))\n",
    "    else:    \n",
    "        size = float(0)\n",
    "        \n",
    "    browser_field = match.group(15)\n",
    "    if browser_field:\n",
    "        browser = browser_field\n",
    "    else:\n",
    "        browser = 'not_found'\n",
    "    \n",
    "    parsed_row = Row(\n",
    "        ip          = match.group(1),\n",
    "        date = datetime.strptime(match.group(2), \"%Y-%m-%d\").date(),\n",
    "        time       = datetime.strptime(match.group(3), \"%H:%M:%S\").time(),\n",
    "        zone        = match.group(4),\n",
    "        cik      = match.group(5),\n",
    "        accesion      = match.group(6),\n",
    "        filename = match.group(7),\n",
    "        response_code = int(float(match.group(8))),\n",
    "        content_size  = size,\n",
    "        idx = bool(match.group(10)),\n",
    "        browser = browser\n",
    "    )\n",
    "    print('parsed   ', logline)\n",
    "    \n",
    "    return (parsed_row, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parsed    208.252.214.jbf,2003-03-03,00:00:00,500.0,919642.0,0000891836-02-000291,-index.htm,304.0,,1.0,0.0,0.0,1.0,0.0,mie\n"
     ]
    }
   ],
   "source": [
    "#parseApacheLogLine('208.252.214.jbf,2003-03-03,00:00:00,500.0,919642.0,0000891836-02-000291,-index.htm,304.0,,1.0,0.0,0.0,1.0,0.0,mie')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parsed    209.172.247.haf,2003-03-03,00:00:04,500.0,78003.0,0000914121-02-001461,pf121702-8k.txt,200.0,2527.0,0.0,0.0,0.0,9.0,0.0,win\n"
     ]
    }
   ],
   "source": [
    "#parseApacheLogLine('209.172.247.haf,2003-03-03,00:00:04,500.0,78003.0,0000914121-02-001461,pf121702-8k.txt,200.0,2527.0,0.0,0.0,0.0,9.0,0.0,win')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parsed    66.48.138.ach,2003-03-03,00:01:52,500.0,1037949.0,0001047469-03-006343,.txt,200.0,13224.0,0.0,1.0,0.0,0.0,0.0,\n"
     ]
    }
   ],
   "source": [
    "#parseApacheLogLine('66.48.138.ach,2003-03-03,00:01:52,500.0,1037949.0,0001047469-03-006343,.txt,200.0,13224.0,0.0,1.0,0.0,0.0,0.0,')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsedLogsRDD1 = logFileRDD.map(parseApacheLogLine).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Row(ip='208.252.214.jbf', date=datetime.date(2003, 3, 3), time=datetime.time(0, 0), zone='500.0', cik='919642.0', accesion='0000891836-02-000291', filename='-index.htm', response_code=304, content_size=0.0, idx=True, browser='mie'),\n",
       "  1)]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsedLogsRDD1.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of invalid logline: 17\n",
      "Invalid logline: 144.214.5.cci,2003-03-03,01:49:11,500.0,313616.0,0000928385-02-001161,0000928385%2D02%2D001161.txt,200.0,150038.0,0.0,0.0,0.0,10.0,0.0,mie\n",
      "Invalid logline: 144.214.5.cci,2003-03-03,01:55:33,500.0,313616.0,0000928385-02-001161,0000928385%2D02%2D001161.txt,200.0,103318.0,0.0,0.0,0.0,10.0,0.0,mie\n",
      "Invalid logline: 12.222.107.fbd,2003-03-03,03:04:45,500.0,18230.0,0000018230-01-500093,0000018230%2D01%2D500093.txt,200.0,396293.0,0.0,0.0,0.0,10.0,0.0,win\n",
      "Invalid logline: 203.218.153.jcd,2003-03-03,03:18:02,500.0,24491.0,0000950152-01-506621,0000950152%2D01%2D506621.txt,200.0,245669.0,0.0,0.0,0.0,10.0,0.0,win\n",
      "Invalid logline: 204.4.131.egd,2003-03-03,06:26:37,500.0,1025773.0,0000912057-96-023543,.txt%20http://www.sonexis.com/company/in,404.0,3451.0,0.0,1.0,0.0,0.0,1.0,win\n",
      "Invalid logline: 202.140.161.ecg,2003-03-03,08:01:34,500.0,310569.0,0000950138-01-500074,0000950138%2D01%2D500074.txt,200.0,116024.0,0.0,0.0,0.0,10.0,0.0,win\n",
      "Invalid logline: 202.125.246.cah,2003-03-03,11:04:37,500.0,21344.0,0000021344-02-000011,0000021344%2D02%2D000011.txt,200.0,90178.0,0.0,0.0,0.0,10.0,0.0,win\n",
      "Invalid logline: 203.80.99.jgj,2003-03-03,11:27:04,500.0,1091862.0,0001091862-02-000007,0001091862%2D02%2D000007.txt,200.0,79958.0,0.0,0.0,0.0,10.0,0.0,mie\n",
      "Invalid logline: 209.131.59.jhd,2003-03-03,14:37:17,500.0,1024302.0,0000891618-03-000702,%20f86711dmdefm14a.htm,404.0,3451.0,0.0,1.0,0.0,0.0,1.0,mie\n",
      "Invalid logline: 65.125.4.ehd,2003-03-03,15:05:20,500.0,1004980.0,0001047469-03-007123,a2103978zex-10%20_23.htm,404.0,3451.0,0.0,1.0,0.0,0.0,1.0,win\n",
      "Invalid logline: 219.76.96.aba,2003-03-03,15:06:37,500.0,18230.0,0000018230-01-500073,0000018230%2D01%2D500073.txt,200.0,441299.0,0.0,0.0,0.0,10.0,0.0,\n",
      "Invalid logline: 208.218.176.jja,2003-03-03,15:12:20,500.0,1004980.0,0001047469-03-007123,a2103978zex-10%20_23.htm,404.0,3451.0,0.0,1.0,0.0,0.0,1.0,win\n",
      "Invalid logline: 208.218.176.jja,2003-03-03,15:12:44,500.0,1004980.0,0001047469-03-007123,a2103978zex-10%20_23.htm,404.0,3451.0,0.0,1.0,0.0,0.0,1.0,win\n",
      "Invalid logline: 63.178.246.gha,2003-03-03,21:56:54,500.0,909954.0,0000909954-03-000003,/proxyforsec.htm,200.0,52723.0,0.0,0.0,0.0,10.0,0.0,\n",
      "Invalid logline: 218.188.21.fgd,2003-03-03,22:02:43,500.0,1072342.0,0000930661-02-000731,0000930661%2D02%2D000731.txt,200.0,121291.0,0.0,0.0,0.0,10.0,0.0,win\n",
      "Invalid logline: 24.214.131.jie,2003-03-03,22:49:10,500.0,1021810.0,0001021810-03-000007,form/10kedgarized.txt,404.0,3451.0,0.0,1.0,0.0,0.0,1.0,win\n",
      "Invalid logline: 199.89.127.fha,2003-03-03,23:35:03,500.0,1004980.0,0001047469-03-007123,a2103978zex-10%20_23.htm,404.0,3451.0,0.0,1.0,0.0,0.0,1.0,win\n",
      "Read 163455 lines, successfully parsed 163438 lines,            failed to parse 17 lines\n"
     ]
    }
   ],
   "source": [
    "accessLogsRDD1, failedLogsRDD1 = access_fail_logs(parsedLogsRDD1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking again that results are meaningful"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the moment I ignore the 17 unparsed lines and check that that parsed responses are meaningful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response codes are [200, 206, 302, 304, 403, 404, 416]\n",
      "Browsers are ['iem', 'lin', 'mac', 'mie', 'not_found', 'opr', 'win']\n"
     ]
    }
   ],
   "source": [
    "distinct_responsecodes_browsers(accessLogsRDD1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again we check that the results are reasonable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Going to  production code\n",
    "\n",
    "    * put the functions in the utils.py \n",
    "    * make parseApacheLogLine(logline) specific for each pattern and keep this in one file patter_EDGAR.py\n",
    "    * logging instead of print statements\n",
    "    * use pytest for testing patterns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-09 22:44:12 INFO     pattern_EDGAR <module> line_35 LOG_PATTERN_EDGAR:  ^(\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\w{3}),(\\d{4}\\-\\d{2}\\-\\d{2}),(\\d{2}:\\d{2}:\\d{2}),(\\d+\\.?\\d*),(\\d+\\.?\\d*),([\\w\\-]+),([\\w\\-\\.]+),(\\d+\\.?\\d*),(\\d*\\.?\\d*),([01]\\.?0?),([01]\\.?0?),([01]\\.?0?),(1?[0-9]\\.?0?),([01]\\.?0?),([a-z]{0,3})$\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<module 'utils' from '/home/olga/github-my/Web-Server-Log-Analysis-with-PySpark/utils.py'>"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "import pattern_EDGAR as pE; importlib.reload(pE)\n",
    "import utils; importlib.reload(utils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "parsedLogsRDD1 = logFileRDD.map(pE.parseApacheLogLine).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Row(ip='208.252.214.jbf', date=datetime.date(2003, 3, 3), time=datetime.time(0, 0), zone='500.0', cik='919642.0', accesion='0000891836-02-000291', filename='-index.htm', response_code=304, content_size=0.0, idx=True, browser='mie'),\n",
       "  1),\n",
       " (Row(ip='66.48.138.ach', date=datetime.date(2003, 3, 3), time=datetime.time(0, 0, 2), zone='500.0', cik='1084230.0', accesion='0001104659-03-003192', filename='.txt', response_code=200, content_size=330570.0, idx=True, browser='not_found'),\n",
       "  1),\n",
       " (Row(ip='209.172.247.haf', date=datetime.date(2003, 3, 3), time=datetime.time(0, 0, 4), zone='500.0', cik='78003.0', accesion='0000914121-02-001461', filename='pf121702-8k.txt', response_code=200, content_size=2527.0, idx=True, browser='win'),\n",
       "  1)]"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsedLogsRDD1.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-09 22:44:29 INFO     utils        access_fail_logs line_28 Number of invalid logline: 17\n",
      "2022-02-09 22:44:29 INFO     utils        access_fail_logs line_30 Invalid logline: 144.214.5.cci,2003-03-03,01:49:11,500.0,313616.0,0000928385-02-001161,0000928385%2D02%2D001161.txt,200.0,150038.0,0.0,0.0,0.0,10.0,0.0,mie\n",
      "2022-02-09 22:44:29 INFO     utils        access_fail_logs line_30 Invalid logline: 144.214.5.cci,2003-03-03,01:55:33,500.0,313616.0,0000928385-02-001161,0000928385%2D02%2D001161.txt,200.0,103318.0,0.0,0.0,0.0,10.0,0.0,mie\n",
      "2022-02-09 22:44:29 INFO     utils        access_fail_logs line_30 Invalid logline: 12.222.107.fbd,2003-03-03,03:04:45,500.0,18230.0,0000018230-01-500093,0000018230%2D01%2D500093.txt,200.0,396293.0,0.0,0.0,0.0,10.0,0.0,win\n",
      "2022-02-09 22:44:29 INFO     utils        access_fail_logs line_30 Invalid logline: 203.218.153.jcd,2003-03-03,03:18:02,500.0,24491.0,0000950152-01-506621,0000950152%2D01%2D506621.txt,200.0,245669.0,0.0,0.0,0.0,10.0,0.0,win\n",
      "2022-02-09 22:44:29 INFO     utils        access_fail_logs line_30 Invalid logline: 204.4.131.egd,2003-03-03,06:26:37,500.0,1025773.0,0000912057-96-023543,.txt%20http://www.sonexis.com/company/in,404.0,3451.0,0.0,1.0,0.0,0.0,1.0,win\n",
      "2022-02-09 22:44:29 INFO     utils        access_fail_logs line_30 Invalid logline: 202.140.161.ecg,2003-03-03,08:01:34,500.0,310569.0,0000950138-01-500074,0000950138%2D01%2D500074.txt,200.0,116024.0,0.0,0.0,0.0,10.0,0.0,win\n",
      "2022-02-09 22:44:29 INFO     utils        access_fail_logs line_30 Invalid logline: 202.125.246.cah,2003-03-03,11:04:37,500.0,21344.0,0000021344-02-000011,0000021344%2D02%2D000011.txt,200.0,90178.0,0.0,0.0,0.0,10.0,0.0,win\n",
      "2022-02-09 22:44:29 INFO     utils        access_fail_logs line_30 Invalid logline: 203.80.99.jgj,2003-03-03,11:27:04,500.0,1091862.0,0001091862-02-000007,0001091862%2D02%2D000007.txt,200.0,79958.0,0.0,0.0,0.0,10.0,0.0,mie\n",
      "2022-02-09 22:44:29 INFO     utils        access_fail_logs line_30 Invalid logline: 209.131.59.jhd,2003-03-03,14:37:17,500.0,1024302.0,0000891618-03-000702,%20f86711dmdefm14a.htm,404.0,3451.0,0.0,1.0,0.0,0.0,1.0,mie\n",
      "2022-02-09 22:44:29 INFO     utils        access_fail_logs line_30 Invalid logline: 65.125.4.ehd,2003-03-03,15:05:20,500.0,1004980.0,0001047469-03-007123,a2103978zex-10%20_23.htm,404.0,3451.0,0.0,1.0,0.0,0.0,1.0,win\n",
      "2022-02-09 22:44:29 INFO     utils        access_fail_logs line_30 Invalid logline: 219.76.96.aba,2003-03-03,15:06:37,500.0,18230.0,0000018230-01-500073,0000018230%2D01%2D500073.txt,200.0,441299.0,0.0,0.0,0.0,10.0,0.0,\n",
      "2022-02-09 22:44:29 INFO     utils        access_fail_logs line_30 Invalid logline: 208.218.176.jja,2003-03-03,15:12:20,500.0,1004980.0,0001047469-03-007123,a2103978zex-10%20_23.htm,404.0,3451.0,0.0,1.0,0.0,0.0,1.0,win\n",
      "2022-02-09 22:44:29 INFO     utils        access_fail_logs line_30 Invalid logline: 208.218.176.jja,2003-03-03,15:12:44,500.0,1004980.0,0001047469-03-007123,a2103978zex-10%20_23.htm,404.0,3451.0,0.0,1.0,0.0,0.0,1.0,win\n",
      "2022-02-09 22:44:29 INFO     utils        access_fail_logs line_30 Invalid logline: 63.178.246.gha,2003-03-03,21:56:54,500.0,909954.0,0000909954-03-000003,/proxyforsec.htm,200.0,52723.0,0.0,0.0,0.0,10.0,0.0,\n",
      "2022-02-09 22:44:29 INFO     utils        access_fail_logs line_30 Invalid logline: 218.188.21.fgd,2003-03-03,22:02:43,500.0,1072342.0,0000930661-02-000731,0000930661%2D02%2D000731.txt,200.0,121291.0,0.0,0.0,0.0,10.0,0.0,win\n",
      "2022-02-09 22:44:29 INFO     utils        access_fail_logs line_30 Invalid logline: 24.214.131.jie,2003-03-03,22:49:10,500.0,1021810.0,0001021810-03-000007,form/10kedgarized.txt,404.0,3451.0,0.0,1.0,0.0,0.0,1.0,win\n",
      "2022-02-09 22:44:29 INFO     utils        access_fail_logs line_30 Invalid logline: 199.89.127.fha,2003-03-03,23:35:03,500.0,1004980.0,0001047469-03-007123,a2103978zex-10%20_23.htm,404.0,3451.0,0.0,1.0,0.0,0.0,1.0,win\n",
      "2022-02-09 22:44:30 INFO     utils        access_fail_logs line_32 Read 163455 lines, successfully parsed 163438 lines,            failed to parse 17 lines\n"
     ]
    }
   ],
   "source": [
    "accessLogsRDD1, failedLogsRDD1 = utils.access_fail_logs(parsedLogsRDD1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "failedLogsRDD1.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-09 22:44:44 DEBUG    pattern_EDGAR distinct_responsecodes_browsers line_91 Response codes are [200, 206, 302, 404]\n",
      "2022-02-09 22:44:44 DEBUG    pattern_EDGAR distinct_responsecodes_browsers line_95 Browsers are ['iem', 'lin', 'mac', 'mie', 'opr', 'win']\n"
     ]
    }
   ],
   "source": [
    "uniqueResponseCodes, uniqueBrowsers = pE.distinct_responsecodes_browsers(accessLogsRDD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uniqueResponseCodes [200, 206, 302, 404]\n",
      "uniqueBrowsers ['iem', 'lin', 'mac', 'mie', 'opr', 'win']\n"
     ]
    }
   ],
   "source": [
    "print('uniqueResponseCodes', uniqueResponseCodes)\n",
    "print('uniqueBrowsers', uniqueBrowsers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "import pattern_EDGAR as pE  #; importlib.reload(pE)\n",
    "import utils;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sc = SparkContext(\"local[2]\", \"NetworkWordCount\")\n",
    "ssc = StreamingContext(sc, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = ssc.socketTextStream(\"localhost\", 8890)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsedLogsRDD1 = logFileRDD.map(pE.parseApacheLogLine).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsedLogsRDD1.pprint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc.start()             # Start the computation\n",
    "ssc.awaitTermination()  # Wait for the computation to terminate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run Netcat as a data server\n",
    "%sh\n",
    "cat ./data/log20030303.csv | nc -u localhost 8890 -w0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
